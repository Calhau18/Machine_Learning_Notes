\documentclass{article}
\title{Machine Learning Notes}
\date{2022}
\author{JoÃ£o Rocha}
	
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools, nccmath}
\usepackage{geometry}
\usepackage{graphicx}
 \geometry{
 a4paper,
 total={129mm,170mm},
 left=23mm,
 top=23mm,
 bottom=28mm,
 right=28mm,
 }

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\begin{document}
\maketitle

\section{Introduction}

\section{Supervised Learning}

We call \textbf{Supervised Learning} to instances of learning where we are given a dataset $(X, Y) \in \R^{(m + n) \times d}$ where $X$ are some inputs and $Y$ are some outputs and each pair $(x, y)$ is an observation or an instance of our dataset. 
The purpose of supervised learning is to learn some mapping that fits the dataset best. 
That is, for some error function $E$, we shall learn a mapping $\hat{f}$ such that $E(\hat{f(X)}, Y)$ is minimized.

\subsection{Linear Regression}

For \textbf{Linear Regression} we assume that the outputs are a linear function of the inputs, and thus our function $\hat{f}$ can be written as $\hat{f}(x) = \theta x$ for some $\theta \in \R^{(m+1)}$.
We define $\theta$ to be in $\R^{(m+1)}$ in order to allow linear functions that do not go through the origin (more precisely, afine functions).
With that in mind, we also extend each input vector $x$ to be in $\R^{(m+1)}$ using the mapping $x \to (1, x)$.

Therefore, we want to find $\hat{\theta} = \arg \min_\theta E(\theta X, Y)$, and our predictions will be of the form $\hat{f}(x) = \hat{\theta}x$.

The most standard way to define $E$ is to use the \textbf{Mean Squared Error} (MSE) defined as 

$$
E(X \theta, Y) = \frac{1}{m} \|X \theta - Y\|^2 = \frac{1}{m} (X \theta - Y)^T (X \theta - Y)
$$

where we assume to be using the Euclidean norm (we explain why bellow).
Equivalently, we can try to minimize the \textbf{Absolute Squared Error} (ASE) that is just the sum of the square differences ($\text{ASE} = m \cdot \text{MSE}$).

Since, we want to minimize this value, we can just differentiate it with respect to $A$ and set it to zero.

\begin{gather*}
\nabla_\theta (E) = 0 \Leftrightarrow 
\nabla_\theta  \left( (X \theta - Y)^T(X \theta - Y) \right) = 0 \Leftrightarrow \\
\nabla_\theta  \left( \theta^T X^T X \theta - \theta^T X^T Y - Y^T X \theta + Y^T Y \right) = 0 \Leftrightarrow \\
2 X^T X \theta - 2 X^T Y = 0 \Leftrightarrow 
\theta = (X^T X)^{-1} X^T Y
\end{gather*}

Note that $X^T X$ is a symmetric matrix, and thus it is invertible if and only if it is positive definite.
This means that we can only use this method if the number of features $m$ is smaller than the number of instances $n$.

The Euclidean norm is the most commonly used norm for measurements in $\R^d$, but that is not why we use it in this instance.
In fact, the Euclidean norm is in some aspects less desirable than other norms (for example, it is very unreliable in outlier detection).
However, it arises naturally by assuming that the errors are normally distributed.
In fact, if we assume that the random variable $X \theta - Y$ is normally distributed with mean zero and variance $\sigma^2$, then the \textbf{Maximum Likelihood Estimation} for $\theta$ is the one that minimizes the MSE.

This predictions algorithm has the downside that only works with data that depends linearly on the inputs.

This approach to linear regression has a complexity of $O(dm^2 + m^3)$ to compute $\theta$, but predicts new values in $O(m)$ time.

\subsubsection{Nonlinear Regression}

Assume our dataset does not fit well to any linear function of the inputs. 
One way to deal with this is to use a nonlinear function of the inputs.
We can think of this as transforming our inputs in $\R^m$ to $\R^k$ through a function $\phi: \R^m \to \R^k$ ($k > m$) and then using linear regression on the new inputs.
This allows us to fit our data to any function in the span of $\phi$.

In this case, $\theta$ is given by
$$
\theta = (\Phi(X)^T \Phi(X))^{-1} \Phi(X)^T Y
$$
where $\Phi(X)$ is the matrix with the $\phi(x_i)$ as rows.
Just like before, we predict according to the function $\hat{f}(x) = \theta^T \phi(x)$.

It is worth noting that our default assumption that $x_0 = 1$ for every vector can be viewed as a particular case of a mapping - $\phi(x) = (1, x)$.

This approach has the problem that it might not be obvious what kind of function our dataset follows.

It computes $\theta$ in $O(f(d,m,k) + dk^2 + k^3)$ time (where $f(d,m,k)$ is the time needed to compute $\Phi$), predicting in $O(k)$ time.

\subsubsection{Locally Weighted Linear Regression}

If the outputs are a continuous function of the inputs, we can always fit a linear function locally to the data at any given point.
We can achieve this by using a weighted linear regression, where the weights are given by a function $w: \R^m \to \R$ that is high for points close to the point we are predicting and low for points far away.
The most commonly used function to assign weights is therefore $w(x) = \exp(-\frac{||x - x_i||^2}{2\tau^2})$ for some $\tau > 0$ ($x_i$ is the point we want to predict).
Therefore, in locally weighted regression, instead of minimizing $||X \theta - Y||^2$, we minimize $||W^{\frac{1}{2}}(X \theta - Y)||^2$ where $W$ is a diagonal matrix with the weights on the diagonal.
A prediction in this case is given by
$$
\hat{f}(x) = \theta^T x = ((X^T W X)^{-1} X^T W^{\frac{1}{2}} Y)^T x
$$

This approach, while working almost always, is very slow, since it needs to compute the weights for every point in the dataset.
It takes the same $O(dm^2 + m^3)$ time to compute $\theta$ as the above methods, but needs to compute a different $\theta$ for every prediction.

\subsection{Logistic Regression}

\textbf{Logistic Regression} is an algorithm used for binary classifications. 
Instead of finding an hyperplane that best fits a real-valued output, we find a hyperplane that best separates positive and negatives observations.
We will then assume that our outputs are in $\{0, 1\}$, and it is reasonable to assume that $p(0 | x ; \theta) + p(1 | x ; \theta) = 1$.
If we define $h_\theta(x) = p(1 | x ; \theta)$, then we can write
$$
p(y | x ; \theta) = h_\theta(x)^y (1 - h_\theta(x))^{1 - y}
$$
Therefore, if do a MLE for $\theta$, we find that 
$$
\hat{\theta} = l(\theta) = \log \mathcal{L}(\theta) = \sum_{i=1}^d y_i \log h_\theta(x_i) + (1 - y_i) \log (1 - h_\theta(x_i))
$$
Now, unlike before, just differentiating with respect to $\theta$ and setting it to zero will not yield any close form solution.
However, assuming $h_\theta(x)$ is increasing, the above function is convex, and we can use the toolset to optimize convex functions to find the MLE estimation for $\theta$.
In fact, the function is strictly convex, which guarantees the existance of an unique global minimum.

We can't not mention the \textbf{Sigmoid Function} $\sigma(z) = \frac{1}{1 + \exp(-z)}$.
The usual choice for the function $h_\theta(x) = \sigma(\theta^T x)$ as determined by the Theory of Generalized Linear Models, which we will touch bellow.

\subsection{Convex Optimization}

Both the error function on linear regression and the loss function on logistic regression are convex functions over a convex set $\R^{m+1}$.
Therefore, we can use usual techniques to solve for the minimal value of these functions.
We present two of the most usual techniques bellow.

\subsubsection{Gradient Descent}

From multivariate calculus, we remember that the gradient of a function $f: \R^n \to \R$ gives us the direction of steepest ascent.
\textbf{Gradient Descent} arises from the idea that moving in the direction of $-\nabla f$ always decreases the value of $f$ (for a sufficiently small "step").
Therefore, the gradient descent algorithm can be briefly described as follows:
\begin{enumerate}
\item Choose a starting point $x_0 \in \R^n$.
\item Repeat until convergence:
\begin{enumerate}
\item Compute $x_{k+1} = x_k - \alpha \nabla f(x_k)$, where $\alpha$ is a small constant.
\end{enumerate}
\end{enumerate}
This algorithm is guaranteed to converge to a local minimum of $f$.
Therefore, in the case of strictly convex functions, it will converge to the global minimum.

Note that the computation of this algorithm can be expensive. 
For example, both in linear and logistic regression, the computation of the gradient takes $O(md)$ time (in the logistic case we assume that $h_\theta(x)$ can be differentiated with respected to $\theta_i$ in $O(1)$ time).
We perform this computation once per update.
Defining $s$ as the number of gradient descent steps until convergence, the complexity of the algorithm is $O(mds)$.

The value of $s$ depends strongly on the choice of $\alpha$.
If $\alpha$ is too small, the algorithm will take a long time to converge.
Note, however, that setting $\alpha$ to a larger number might cause the algorithm to "overshoot" the intended minimum, stopping it from converging.
The choice of $\alpha$ is therefore a delicate balance, and it is usually chosen by trial and error.
A frequent option is to start with a large $\alpha$ and decrease it as the algorithm converges.

\subsubsection{Newton's Method}

\textbf{Newton's Method} is an algorithm to find the roots of a function $f: \R^n \to \R$.
It can be used to find the global minima of a convex function $f$ by finding the root of $\nabla f$.
The algorithm is as follows:
\begin{enumerate}
\item Choose a starting point $x_0 \in \R^n$.
\item Repeat until convergence:
\begin{enumerate}
\item Compute $x_{k+1} = x_k - H_f(x_k)^{-1} \nabla f(x_k)$
\end{enumerate}
\end{enumerate}

Assuming that $f$ is strictly convex and $C^2(I)$ and our initial guess is in $I$ (in fact the conditions are more general, but for our case this is enough), Newton's Method is guaranteed to converge quadratically - that is, for a root $z$ and an iteration $x_n$, the error $\epsilon_n = ||z - x_n||$ satisfies
$$
\epsilon_{n+1} \leq M \epsilon_n^2
$$
for some scalar $M$.

Newton's Method converges much faster than Gradient Descent, but each step is also much slower to compute.
In linear and logistic regression, computing the gradient of, respectively, the error and log-likelihood functions takes $O(md)$.
Computing the hessian can be done in $O(m^2d)$ time, and computing its inverse takes $O(m^3)$ giving us a time complexity of $O(m^3 + m^2d)$ per step.
Therefore, the overall complexity is $O(m^2(m+d)s)$.

May it be the case that we cannot be sure of an initial point under which the convergence conditions are met, we might want to use Gradient Descent until this is the case.

\subsection{Generalized Linear Models}

\subsection{Perceptron}

The \textbf{perceptron} algorithm is a simple algorithm for binary classification that tries to find a linear function of the inputs that separates the positive and negative outputs.
It classifies every input $x$ with the function $sgn(w^T x)$ for a parameter $w \in \R^n$.
The sign function is called the \textbf{activation function}.
This parameter is found by the \textbf{perceptron learning rule}:
\begin{enumerate}
\item Choose a starting point $w_0 \in \R^n$.
\item Choose a learning rate $\eta \in ]0,1[$.
\item Repeat until convergence:
\begin{enumerate}
\item Choose an observation $(x_i, y_i)$ from the dataset
\item Compute $\Delta w = \eta (y_i - o_i) x_i$, where $o_i = sgn(w^T x_i)$.
\item Update the weights $w = w + \Delta w$.
\end{enumerate}
\end{enumerate}

The algorithm is guaranteed to converge to a solution if the data is linearly separable and $\eta$ is sufficiently small.

Note the similarity between the perceptron algorithm and logistic regression - the only difference between the two is in the so called \textbf{activation function}: the perceptron uses the $sgn$ function, while logistic regression uses the sigmoid function.
Furthermore, it is worth noting that since the derivative of the sign function is 0 at every point (except 0, where it is not defined), we cannot use gradient descent to "teach" a perceptron, creating the need for a specific learning algorithm.

\subsection{Neural Networks}

A \textbf{Neural Network} can be described as a set of \textbf{nodes}, organized in layers such that each node stores a value $y \in \R$, that depends on the nodes of the previous layers $x \in \R^n$ as $y = f(w^T x + b)$ for a parameter $w \in \R^n$ and some activation function $f: \R \to \R$.
Neural networks have a lot of values and variables so it is important to state the notation clearly:
\begin{itemize}
\item $L$ is the number of layers in the network, including the \textbf{input} layer (first layer) and the \textbf{output} layer (last layer). The remaining layers can be refered to as \textbf{hidden layers}.
\item We use $n^{[l]}$ to denote the number of nodes in layer $l$.
\item We denote the values on the nodes of the $l$-th layer as $x^{[l]} \in \R^{n^{[l]}}$. $x^{[1]}$ is the input $x$ and $x^{[L]}$ is the output $\hat{y}$. 
\item As we've stated, we have $x^{[l]}_i = f(w^T x^{[l-1]} + b)$ for $i = 1, \dots, n^{[l]}$ and some weight parameter $w \in \R^{n^{[l]}}$ and some bias $b \in \R$. We use $w^{[l-1]}_i$ and $b^{[l-1]}_i$ to refer to the weights and bias used at the node $x^{[l]}_i$. Therefore, $x^{[l]}_i = f({w^{[l-1]}_i}^T x^{[l-1]} + b^{[l-1]}_i)$. 
\item We usually combine these weights in a weight matrix $W^{[l]} \in \R^{n^{[l+1]} \times n^{[l]}}$ such that the row $i$ of $W$ is ${w^{[l]}_i}^T$. We also combine the biases in a column vector $b \in \R^{n^{[l]}} = [b^{[l+1]}_i]_{i=1}^{n^{[l]}}$. We can then say that $x^{[l]} = f(W^{[l-1]}x^{[l-1]} + b^{[l-1]})$ (we extend the application function to be applied elementwise to a vector).
\item It is useful to define $z^{[l]} = W^{[l-1]}x^{[l-1]} + b^{[l-1]}$. Therefore, $x^{[l]} = f(z^{[l]})$.
\end{itemize}

The prediction process of a neural network, given an input $x$ is then as follows:
\begin{enumerate}
\item Set $x^{[1]} = x$.
\item For $l = 2, \dots, L$:
\begin{enumerate}
\item Compute $z^{[l]} = W^{[l-1]}x^{[l-1]} + b^{[l-1]}$.
\item Compute $x^{[l]} = f(z^{[l]})$.
\end{enumerate}
\item Return $\hat{y} = x^{[L]}$.
\end{enumerate}

This process is often called \textbf{Forward Propragation}, as the input is propagated forward through the network.

We note that all the weight matrices and bias vectors parametrize the neural network.
These are the values that we want to learn from the data.
We can see that the number of parameters is
$$
\sum_{l=1}^{L-1} n^{[l+1]} n^{[l]} + \sum_{l=1}^{L-1} n^{[l]}
$$
assuming that the input vector is of size $n$, the output vector is of size $m$, and each hidden layer is of size in the same order of magnitude (lets say approx. $d$), we conclude that we want to learn $O(nd + Ld^2 + md)$ parameters.

The only thing remaining is then to find a way to learn these parameters.
We can do this by using the \textbf{Backpropagation} algorithm, which is essentially a form of gradient descent.
We will update each parameter as follows:
\begin{gather*}
W^{[l]} \leftarrow W^{[l]} - \eta \frac{\partial L}{\partial W^{[l]}} \\
b^{[l]} \leftarrow b^{[l]} - \eta \frac{\partial L}{\partial b^{[l]}}
\end{gather*}
where $\eta$ is the learning rate.

We see that, for the last layer
\begin{gather*}
\frac{\partial L}{\partial W^{[L-1]}} = \frac{\partial L}{\partial x^{[L]}} \frac{\partial x^{[L]}}{\partial z^{[L]}} \frac{\partial z^{[L]}}{\partial W^{[L-1]}} \\
\frac{\partial L}{\partial b^{[L-1]}} = \frac{\partial L}{\partial x^{[L]}} \frac{\partial x^{[L]}}{\partial z^{[L]}} \frac{\partial z^{[L]}}{\partial b^{[L-1]}}
\end{gather*}

and for any other layer
\begin{gather*}
\frac{\partial L}{\partial W^{[l-1]}} = \frac{\partial L}{\partial x^{[l]}} \frac{\partial x^{[l]}}{\partial z^{[l]}} \frac{\partial z^{[l]}}{\partial x^{[l-1]}} \frac{\partial x^{[l-1]}}{\partial z^{[l-1]}} \frac{\partial z^{[l-1]}}{\partial W^{[l-1]}} \\
\frac{\partial L}{\partial b^{[l-1]}} = \frac{\partial L}{\partial x^{[l]}} \frac{\partial x^{[l]}}{\partial z^{[l]}} \frac{\partial z^{[l]}}{\partial x^{[l-1]}} \frac{\partial x^{[l-1]}}{\partial z^{[l-1]}} \frac{\partial z^{[l-1]}}{\partial b^{[l-1]}}
\end{gather*}

and we can easily compute

\begin{gather*}
\frac{\partial x^{[l]}}{\partial z^{[l]}} = f'(z^{[l]}) \quad
\frac{\partial z^{[l]}}{\partial W^{[l-1]}} = x^{[l-1]} \quad
\frac{\partial z^{[l]}}{\partial b^{[l-1]}} = 1 \quad
\frac{\partial z^{[l]}}{\partial x^{[l-1]}} = W^{[l-1]}
\end{gather*}

We note that the first two terms of the above equations are computed before we reach the corresponding layer. 
Therefore we can reuse these values.
Defining $\delta^{[l]} = \frac{\partial L}{\partial x^{[l]}} \frac{\partial x^{[l]}}{\partial z^{[l]}}$ we can write the above equations as

\begin{gather*}
\frac{\partial L}{\partial W^{[l-1]}} = \delta^{[l]} x^{[l-1]} \\
\frac{\partial L}{\partial b^{[l-1]}} = \delta^{[l]} \\
\delta^{[l-1]} = \delta^{[l]} W^{[l-1]} f'(z^{[l]})
\end{gather*}

Therefore, we can now describe the backpropagation algorithm as follows:
\begin{enumerate}
\item Set $x^{[1]} = x$.
\item Forward progragate the input through the network, computing $z^{[l]}$ and $x^{[l]}$ for each layer.
\item Compute $\delta^{[L]} = \frac{\partial L}{\partial x^{[L]}} \frac{\partial x^{[L]}}{\partial z^{[L]}} = \frac{\partial L}{\partial x^{[L]}} f'(z^{[L]})$.
\item For $l = L-1, \dots, 1$:
\begin{enumerate}
\item Compute $\frac{\partial L}{\partial W^{[l]}} = \delta^{[l+1]} x^{[l]}$.
\item Compute $\frac{\partial L}{\partial b^{[l]}} = \delta^{[l+1]}$.
\item Update the parameters as follows:
\begin{gather*}
W^{[l]} \leftarrow W^{[l]} - \eta \frac{\partial L}{\partial W^{[l]}} \\
b^{[l]} \leftarrow b^{[l]} - \eta \frac{\partial L}{\partial b^{[l]}}
\end{gather*}
\item Compute $\delta^{[l-1]} = \delta^{[l]} W^{[l-1]} f'(z^{[l-1]})$.
\end{enumerate}
\end{enumerate}

So far we have abstracted away the details of the activation function $f$ and the loss function $L$.
These will depend on the problem at hand:
\begin{itemize}
\item For binary classification problems, the most common choices are by far the cross-entropy loss function $L(w) = y \log(\hat{y}) + (1-y) \log(1-\hat{y})$ and the logistic activation function $\sigma(z) = \frac{1}{1+e^{-z}}$.
\item For regression problems, the most common choices are the mean squared error loss function $L(w) = \frac{1}{2} (y-\hat{y})^2$ and the \textbf{Rectified Linear Unit} activation function $RELU(z) = \max(0,z)$.
\end{itemize}

We will repeat the algorithm, now specifying the logistic activation function and the sum of squares loss function.
\begin{enumerate}
\item Set $x^{[1]} = x$.
\item Forward progragate the input through the network, computing $z^{[l]}$ and $x^{[l]}$ for each layer.
\item Compute $\delta^{[L]} = \frac{\partial L}{\partial x^{[L]}} \frac{\partial x^{[L]}}{\partial z^{[L]}} = \frac{\partial L}{\partial x^{[L]}} f'(z^{[L]}) = (\hat{y}-y) f'(z^{[L]}) = (\hat{y}-y) \hat{y}(1-\hat{y})$.
\item For $l = L-1, \dots, 1$:
\begin{enumerate}
\item Compute $\frac{\partial L}{\partial W^{[l]}} = \delta^{[l+1]} x^{[l]}$.
\item Compute $\frac{\partial L}{\partial b^{[l]}} = \delta^{[l+1]}$.
\item Update the parameters as follows:
\begin{gather*}
W^{[l]} \leftarrow W^{[l]} - \eta \frac{\partial L}{\partial W^{[l]}} \\
b^{[l]} \leftarrow b^{[l]} - \eta \frac{\partial L}{\partial b^{[l]}}
\end{gather*}
\item Compute $\delta^{[l-1]} = \delta^{[l]} W^{[l-1]} \sigma(z^{[l]})(1-\sigma(z^{[l]})) = \delta^{[l]}W^{l-1]}x^{[l]}(1-x^{[l]})$.
\end{enumerate}
\end{enumerate}

(we used the fact that $\sigma'(z) = \sigma(z)(1-\sigma(z))$).

\subsection{Naive Bayes}

\subsection{k Nearest Neighbours}

\subsection{Decision Trees}

Some classification problems are best modeled by a complex combination of predicates.
\textbf{Decision Trees} work well in cases where differentiating the classes can be done (or approximated well enough) by asking a series of questions.

A \textbf{Decision Tree} is a tree where each node splits the domain into sections according to a certain predicate on one of the features.
At every leave, we end up with a subset of the domain, and we can assign a class to that subset.
This is done by just taking the mode of the classes in that subset.

The obvious question that poses itself is how to select the feature at each node, and which predicate to evaluate to create the children of the current node.

We start by defining how to choose the predicate.
Normally, the predicate used is just a threshold on the value of the feature, which can be viewed as slicing the domain space at the current node in two, through an hyperplane orthogonal to the axis of the current feature.
To chooose where this slice should be done, we need to define a cost function that measures how good a split is.

\section{Evaluating Models}

\section{Unsupervised Learning}

We say we are in an \textbf{unsupervised learning} setting if we are given a dataset $X$ and we want to gather some information of any form about that data.

\subsection{Clustering}

One of the most common types of unsupervised learning is \textbf{clustering}, where we want to group together the data points in $X$ into clusters.
We define clusters as being subsets of $X$ such that the intradistances are small and the interdistances are large.

\subsubsection{K-Means}

One of the most common algorithm for clustering is \textbf{K-Means}, which works as follows:
\begin{enumerate}
\item Choose $k$ random points in $X$ as the initial centroids $\mu_1, \cdots, \mu_k$.
\item Repeat until convergence
\begin{enumerate}
\item For each point $x \in X$, set $c^{(i)} = \arg\min_{j} ||x-\mu_j||^2$.
\item For each cluster $j$, set $\mu_j = \frac{1}{|C_j|} \sum_{x \in C_j} x$.
\end{enumerate}
\end{enumerate}

\subsubsection{Expectation Maximization (EM)}

= GDA mas sem observar a random variable Y

For the \textbf{Expectation Maximization} algorithm, we assume that there is a random variable $Z$ that is unobservable, such that $Z \sim multinoulli(\phi)$ for some $\phi = (\phi_1, \cdots, \phi_k)$ such that $\sum_{i=1}^k \phi_i = 1$ and 
$$
X \mid Z = j \sim N(\mu_j, \Sigma_j)
$$

The EM algorithm aims to find the maximum likelihood estimate of the parameters $\phi, \mu_1, \cdots, \mu_k, \Sigma_1, \cdots, \Sigma_k$.

The algorithm has to steps:
\begin{enumerate}
\item \textbf{E-step}: compute the posterior distribution of $Z$ given the current parameters:
\begin{gather*}
P(Z = j | X ; \phi, \mu, \Sigma) =
\frac{P(X | Z = j ; \mu, \Sigma) P(Z = j ; \phi)}{\sum_{j=1}^k P(X | Z = j ; \mu, \Sigma) P(Z = j ; \phi)} = 
\frac{\phi_j \mathcal{N}(X ; \mu_j, \Sigma_j)}{\sum_{j=1}^k \phi_j \mathcal{N}(X ; \mu_j, \Sigma_j)}
\end{gather*}
\item \textbf{M-step}: compute the maximum likelihood estimate of the parameters given the current posterior distribution of $Z$:
\begin{gather*}
\phi_j = \frac{1}{N} \sum_{i=1}^N P(Z = j | X ; \phi, \mu, \Sigma) \\
\mu_j = \frac{\sum_{i=1}^N P(Z = j | X ; \phi, \mu, \Sigma) X_i}{\sum_{i=1}^N P(Z = j | X ; \phi, \mu, \Sigma)} \\
\Sigma_j = \frac{\sum_{i=1}^N P(Z = j | X ; \phi, \mu, \Sigma) (X_i - \mu_j)(X_i - \mu_j)^T}{\sum_{i=1}^N P(Z = j | X ; \phi, \mu, \Sigma)}
\end{gather*}

The algorithm is derived by:
\begin{itemize}
\item \textbf{E-step}: Using Jensen's inequality* to obtain a lower bound on the log-likelihood function of the parameters, such that this function and the estimation coincide at the current value of the parameters.
\item \textbf{M-step}: Computing a MLE of the parameters on that lower bound.
\end{itemize}

It is easy to see that iterations of these two steps will converge too a local maximum of the log-likelihood function.

*we use a trick to make the inequality work: we multiply and divide by the probability density function of some distribution $Q_i$ evaluated at one of the classes $z$ from $Z$, and use that to get an expected value in the formula.
We then use the fact that we want our estimation and the log-likelihood function to be equal at the current value of our parameter to conclude that this distribution shall be $Z | X ; \theta$ where $\theta$ are our parameters.

\end{enumerate}

\subsection{Dimensionality Reduction}

\end{document}
