\documentclass{article}
\title{Machine Learning Notes}
\date{2022}
\author{JoÃ£o Rocha}
	
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools, nccmath}
\usepackage{geometry}
\usepackage{graphicx}
 \geometry{
 a4paper,
 total={129mm,170mm},
 left=23mm,
 top=23mm,
 bottom=28mm,
 right=28mm,
 }

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\begin{document}
\maketitle

\section{Introduction}

\section{Supervised Learning}

We call \textbf{Supervised Learning} to instances of learning where we are given a dataset $(X, Y) \in \R^{(m + n) \times d}$ where $X$ are some inputs and $Y$ are some outputs and each pair $(x, y)$ is an observation or an instance of our dataset. 
The purpose of supervised learning is to learn some mapping that fits the dataset best. 
That is, for some error function $E$, we shall learn a mapping $\hat{f}$ such that $E(\hat{f(X)}, Y)$ is minimized.

\subsection{Linear Regression}

For \textbf{Linear Regression} we assume that the outputs are a linear function of the inputs, and thus our function $\hat{f}$ can be written as $\hat{f}(x) = \theta x$ for some $\theta \in \R^{(m+1)}$.
We define $\theta$ to be in $\R^{(m+1)}$ in order to allow linear functions that do not go through the origin (more precisely, afine functions).
With that in mind, we also extend each input vector $x$ to be in $\R^{(m+1)}$ using the mapping $x \to (1, x)$.

Therefore, we want to find $\hat{\theta} = \arg \min_\theta E(\theta X, Y)$, and our predictions will be of the form $\hat{f}(x) = \hat{\theta}x$.

The most standard way to define $E$ is to use the \textbf{Mean Squared Error} (MSE) defined as 

$$
E(X \theta, Y) = \frac{1}{m} \|X \theta - Y\|^2 = \frac{1}{m} (X \theta - Y)^T (X \theta - Y)
$$

where we assume to be using the Euclidean norm (we explain why bellow).
Equivalently, we can try to minimize the \textbf{Absolute Squared Error} (ASE) that is just the sum of the square differences ($\text{ASE} = m \cdot \text{MSE}$).

Since, we want to minimize this value, we can just differentiate it with respect to $A$ and set it to zero.

\begin{gather*}
\nabla_\theta (E) = 0 \Leftrightarrow 
\nabla_\theta  \left( (X \theta - Y)^T(X \theta - Y) \right) = 0 \Leftrightarrow \\
\nabla_\theta  \left( \theta^T X^T X \theta - \theta^T X^T Y - Y^T X \theta + Y^T Y \right) = 0 \Leftrightarrow \\
2 X^T X \theta - 2 X^T Y = 0 \Leftrightarrow 
\theta = (X^T X)^{-1} X^T Y
\end{gather*}

Note that $X^T X$ is a symmetric matrix, and thus it is invertible if and only if it is positive definite.
This means that we can only use this method if the number of features $m$ is smaller than the number of instances $n$.

The Euclidean norm is the most commonly used norm for measurements in $\R^d$, but that is not why we use it in this instance.
In fact, the Euclidean norm is in some aspects less desirable than other norms (for example, it is very unreliable in outlier detection).
However, it arises naturally by assuming that the errors are normally distributed.
In fact, if we assume that the random variable $X \theta - Y$ is normally distributed with mean zero and variance $\sigma^2$, then the \textbf{Maximum Likelihood Estimation} for $\theta$ is the one that minimizes the MSE.

This predictions algorithm has the downside that only works with data that depends linearly on the inputs.

This approach to linear regression has a complexity of $O(dm^2 + m^3)$ to compute $\theta$, but predicts new values in $O(m)$ time.

\subsubsection{Nonlinear Regression}

Assume our dataset does not fit well to any linear function of the inputs. 
One way to deal with this is to use a nonlinear function of the inputs.
We can think of this as transforming our inputs in $\R^m$ to $\R^k$ through a function $\phi: \R^m \to \R^k$ ($k > m$) and then using linear regression on the new inputs.
This allows us to fit our data to any function in the span of $\phi$.

In this case, $\theta$ is given by
$$
\theta = (\Phi(X)^T \Phi(X))^{-1} \Phi(X)^T Y
$$
where $\Phi(X)$ is the matrix with the $\phi(x_i)$ as rows.
Just like before, we predict according to the function $\hat{f}(x) = \theta^T \phi(x)$.

It is worth noting that our default assumption that $x_0 = 1$ for every vector can be viewed as a particular case of a mapping - $\phi(x) = (1, x)$.

This approach has the problem that it might not be obvious what kind of function our dataset follows.

It computes $\theta$ in $O(f(d,m,k) + dk^2 + k^3)$ time (where $f(d,m,k)$ is the time needed to compute $\Phi$), predicting in $O(k)$ time.

\subsubsection{Locally Weighted Linear Regression}

If the outputs are a continuous function of the inputs, we can always fit a linear function locally to the data at any given point.
We can achieve this by using a weighted linear regression, where the weights are given by a function $w: \R^m \to \R$ that is high for points close to the point we are predicting and low for points far away.
The most commonly used function to assign weights is therefore $w(x) = \exp(-\frac{||x - x_i||^2}{2\tau^2})$ for some $\tau > 0$ ($x_i$ is the point we want to predict).
Therefore, in locally weighted regression, instead of minimizing $||X \theta - Y||^2$, we minimize $||W^{\frac{1}{2}}(X \theta - Y)||^2$ where $W$ is a diagonal matrix with the weights on the diagonal.
A prediction in this case is given by
$$
\hat{f}(x) = \theta^T x = ((X^T W X)^{-1} X^T W^{\frac{1}{2}} Y)^T x
$$

This approach, while working almost always, is very slow, since it needs to compute the weights for every point in the dataset.
It takes the same $O(dm^2 + m^3)$ time to compute $\theta$ as the above methods, but needs to compute a different $\theta$ for every prediction.

\subsection{Gradient Descent}

\subsection{Logistic Regression}

\end{document}
