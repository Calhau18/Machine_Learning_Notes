\documentclass{article}
\title{Machine Learning Notes}
\date{2022}
\author{JoÃ£o Rocha}
	
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools, nccmath}
\usepackage{geometry}
\usepackage{graphicx}
 \geometry{
 a4paper,
 total={129mm,170mm},
 left=23mm,
 top=23mm,
 bottom=28mm,
 right=28mm,
 }

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

\begin{document}
\maketitle

\section{Introduction}

\section{Supervised Learning}

We call \textbf{Supervised Learning} to instances of learning where we are given a dataset $(X, Y) \in \R^{(m + n) \times d}$ where $X$ are some inputs and $Y$ are some outputs and each pair $(x, y)$ is an observation or an instance of our dataset. 
The purpose of supervised learning is to learn some mapping that fits the dataset best. 
That is, for some error function $E$, we shall learn a mapping $\hat{f}$ such that $E(\hat{f(X)}, Y)$ is minimized.

\subsection{Regression}

\subsubsection{Linear Regression}

For \textbf{Linear Regression} we assume that the outputs are a linear function of the inputs, and thus our function $\hat{f}$ can be written as $\hat{f}(x) = \theta x$ for some $\theta \in \R^{(m+1)}$.
We define $\theta$ to be in $\R^{(m+1)}$ in order to allow linear functions that do not go through the origin (more precisely, afine functions).
With that in mind, we also extend each input vector $x$ to be in $\R^{(m+1)}$ using the mapping $x \to (1, x)$.

Therefore, we want to find $\hat{\theta} = \arg \min_\theta E(\theta X, Y)$, and our predictions will be of the form $\hat{f}(x) = \hat{\theta}x$.

The most standard way to define $E$ is to use the \textbf{Mean Squared Error} (MSE) defined as 

$$
E(X \theta, Y) = \frac{1}{m} \|X \theta - Y\|^2 = \frac{1}{m} (X \theta - Y)^T (X \theta - Y)
$$

where we assume to be using the Euclidean norm (we explain why bellow).
Equivalently, we can try to minimize the \textbf{Absolute Squared Error} (ASE) that is just the sum of the square differences ($\text{ASE} = m \cdot \text{MSE}$).

Since, we want to minimize this value, we can just differentiate it with respect to $A$ and set it to zero.

\begin{gather*}
\nabla_\theta (E) = 0 \Leftrightarrow 
\nabla_\theta  \left( (X \theta - Y)^T(X \theta - Y) \right) = 0 \Leftrightarrow \\
\nabla_\theta  \left( \theta^T X^T X \theta - \theta^T X^T Y - Y^T X \theta + Y^T Y \right) = 0 \Leftrightarrow \\
2 X^T X \theta - 2 X^T Y = 0 \Leftrightarrow 
\theta = (X^T X)^{-1} X^T Y
\end{gather*}

Note that $X^T X$ is a symmetric matrix, and thus it is invertible if and only if it is positive definite.
This means that we can only use this method if the number of features $m$ is smaller than the number of instances $n$.

The Euclidean norm is the most commonly used norm for measurements in $\R^d$, but that is not why we use it in this instance.
In fact, the Euclidean norm is in some aspects less desirable than other norms (for example, it is very unreliable in outlier detection).
However, it arises naturally by assuming that the errors are normally distributed.
In fact, if we assume that the random variable $X \theta - Y$ is normally distributed with mean zero and variance $\sigma^2$, then the \textbf{Maximum Likelihood Estimation} for $\theta$ is the one that minimizes the MSE.

This predictions algorithm has the downside that only works with data that depends linearly on the inputs.

This approach to linear regression has a complexity of $O(dm^2 + m^3)$ to compute $\theta$, but predicts new values in $O(m)$ time.

\subsubsection{Nonlinear Regression}

Assume our dataset does not fit well to any linear function of the inputs. 
One way to deal with this is to use a nonlinear function of the inputs.
We can think of this as transforming our inputs in $\R^m$ to $\R^k$ through a function $\phi: \R^m \to \R^k$ ($k > m$) and then using linear regression on the new inputs.
This allows us to fit our data to any function in the span of $\phi$.

In this case, $\theta$ is given by
$$
\theta = (\Phi(X)^T \Phi(X))^{-1} \Phi(X)^T Y
$$
where $\Phi(X)$ is the matrix with the $\phi(x_i)$ as rows.
Just like before, we predict according to the function $\hat{f}(x) = \theta^T \phi(x)$.

It is worth noting that our default assumption that $x_0 = 1$ for every vector can be viewed as a particular case of a mapping - $\phi(x) = (1, x)$.

This approach has the problem that it might not be obvious what kind of function our dataset follows.

It computes $\theta$ in $O(f(d,m,k) + dk^2 + k^3)$ time (where $f(d,m,k)$ is the time needed to compute $\Phi$), predicting in $O(k)$ time.

\subsubsection{Locally Weighted Linear Regression}

If the outputs are a continuous function of the inputs, we can always fit a linear function locally to the data at any given point.
We can achieve this by using a weighted linear regression, where the weights are given by a function $w: \R^m \to \R$ that is high for points close to the point we are predicting and low for points far away.
The most commonly used function to assign weights is therefore $w(x) = \exp(-\frac{||x - x_i||^2}{2\tau^2})$ for some $\tau > 0$ ($x_i$ is the point we want to predict).
Therefore, in locally weighted regression, instead of minimizing $||X \theta - Y||^2$, we minimize $||W^{\frac{1}{2}}(X \theta - Y)||^2$ where $W$ is a diagonal matrix with the weights on the diagonal.
A prediction in this case is given by
$$
\hat{f}(x) = \theta^T x = ((X^T W X)^{-1} X^T W^{\frac{1}{2}} Y)^T x
$$

This approach, while working almost always, is very slow, since it needs to compute the weights for every point in the dataset.
It takes the same $O(dm^2 + m^3)$ time to compute $\theta$ as the above methods, but needs to compute a different $\theta$ for every prediction.

\subsection{Classification}

\subsubsection{Logistic Regression}

\textbf{Logistic Regression} is an algorithm used for binary classifications. 
Instead of finding an hyperplane that best fits a real-valued output, we find a hyperplane that best separates positive and negatives observations.
We will then assume that our outputs are in $\{0, 1\}$, and it is reasonable to assume that $p(0 | x ; \theta) + p(1 | x ; \theta) = 1$.
If we define $h_\theta(x) = p(1 | x ; \theta)$, then we can write
$$
p(y | x ; \theta) = h_\theta(x)^y (1 - h_\theta(x))^{1 - y}
$$
Therefore, if do a MLE for $\theta$, we find that 
$$
\hat{\theta} = l(\theta) = \log \mathcal{L}(\theta) = \sum_{i=1}^d y_i \log h_\theta(x_i) + (1 - y_i) \log (1 - h_\theta(x_i))
$$
Now, unlike before, just differentiating with respect to $\theta$ and setting it to zero will not yield any close form solution.
However, assuming $h_\theta(x)$ is increasing, the above function is convex, and we can use the toolset to optimize convex functions to find the MLE estimation for $\theta$.
In fact, the function is strictly convex, which guarantees the existance of an unique global minimum.

We can't not mention the \textbf{Sigmoid Function} $\sigma(z) = \frac{1}{1 + \exp(-z)}$.
The usual choice for the function $h_\theta(x) = \sigma(\theta^T x)$ as determined by the Theory of Generalized Linear Models, which we will touch bellow.

\subsubsection{Naive Bayes}

\subsubsection{k Nearest Neighbours}

\subsubsection{Decision Trees}

\subsection{Convex Optimization}

Both the error function on linear regression and the loss function on logistic regression are convex functions over a convex set $\R^{m+1}$.
Therefore, we can use usual techniques to solve for the minimal value of these functions.
We present two of the most usual techniques bellow.

\subsubsection{Gradient Descent}

From multivariate calculus, we remember that the gradient of a function $f: \R^n \to \R$ gives us the direction of steepest ascent.
\textbf{Gradient Descent} arises from the idea that moving in the direction of $-\nabla f$ always decreases the value of $f$ (for a sufficiently small "step").
Therefore, the gradient descent algorithm can be briefly described as follows:
\begin{enumerate}
\item Choose a starting point $x_0 \in \R^n$.
\item Repeat until convergence:
\begin{enumerate}
\item Compute $x_{k+1} = x_k - \alpha \nabla f(x_k)$, where $\alpha$ is a small constant.
\end{enumerate}
\end{enumerate}
This algorithm is guaranteed to converge to a local minimum of $f$.
Therefore, in the case of strictly convex functions, it will converge to the global minimum.

Note that the computation of this algorithm can be expensive. 
For example, both in linear and logistic regression, the computation of the gradient takes $O(md)$ time (in the logistic case we assume that $h_\theta(x)$ can be differentiated with respected to $\theta_i$ in $O(1)$ time).
We perform this computation once per update.
Defining $s$ as the number of gradient descent steps until convergence, the complexity of the algorithm is $O(mds)$.

The value of $s$ depends strongly on the choice of $\alpha$.
If $\alpha$ is too small, the algorithm will take a long time to converge.
Note, however, that setting $\alpha$ to a larger number might cause the algorithm to "overshoot" the intended minimum, stopping it from converging.
The choice of $\alpha$ is therefore a delicate balance, and it is usually chosen by trial and error.
A frequent option is to start with a large $\alpha$ and decrease it as the algorithm converges.

\subsubsection{Newton's Method}

\textbf{Newton's Method} is an algorithm to find the roots of a function $f: \R^n \to \R$.
It can be used to find the global minima of a convex function $f$ by finding the root of $\nabla f$.
The algorithm is as follows:
\begin{enumerate}
\item Choose a starting point $x_0 \in \R^n$.
\item Repeat until convergence:
\begin{enumerate}
\item Compute $x_{k+1} = x_k - H_f(x_k)^{-1} \nabla f(x_k)$
\end{enumerate}
\end{enumerate}

Assuming that $f$ is strictly convex and $C^2(I)$ and our initial guess is in $I$ (in fact the conditions are more general, but for our case this is enough), Newton's Method is guaranteed to converge quadratically - that is, for a root $z$ and an iteration $x_n$, the error $\epsilon_n = ||z - x_n||$ satisfies
$$
\epsilon_{n+1} \leq M \epsilon_n^2
$$
for some scalar $M$.

Newton's Method converges much faster than Gradient Descent, but each step is also much slower to compute.
In linear and logistic regression, computing the gradient of, respectively, the error and log-likelihood functions takes $O(md)$.
Computing the hessian can be done in $O(m^2d)$ time, and computing its inverse takes $O(m^3)$ giving us a time complexity of $O(m^3 + m^2d)$ per step.
Therefore, the overall complexity is $O(m^2(m+d)s)$.

May it be the case that we cannot be sure of an initial point under which the convergence conditions are met, we might want to use Gradient Descent until this is the case.

\subsection{Generalized Linear Models}

\section{Evaluating Models}

\section{Unsupervised Learning}

\end{document}
